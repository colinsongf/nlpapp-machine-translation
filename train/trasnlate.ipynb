{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:30:40.319601Z",
     "start_time": "2019-11-12T09:30:40.311265Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os, json, codecs\n",
    "import tensorflow as tf\n",
    "from bert4keras.bert import build_bert_model\n",
    "from bert4keras.utils import Tokenizer, load_vocab, parallel_apply\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:30:41.734410Z",
     "start_time": "2019-11-12T09:30:41.731509Z"
    }
   },
   "outputs": [],
   "source": [
    "config_path = './multilingual_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = './multilingual_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = './multilingual_L-12_H-768_A-12/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:30:42.411741Z",
     "start_time": "2019-11-12T09:30:42.389205Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_token_dict(token_file):\n",
    "    with open(token_file,\"r\") as f:\n",
    "        token_list = f.readlines()\n",
    "        token_dict = {word.strip():id_ for id_,word in enumerate(token_list)}\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "# class OurTokenizer(Tokenizer):\n",
    "#     def _tokenize(self, text):\n",
    "#         R = []\n",
    "#         for c in text:\n",
    "#             if c in self._token_dict:\n",
    "#                 R.append(c)\n",
    "#             elif self._is_space(c):\n",
    "#                 R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "#             else:\n",
    "#                 R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "#         return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = get_token_dict(dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:30:44.263110Z",
     "start_time": "2019-11-12T09:30:44.259173Z"
    }
   },
   "outputs": [],
   "source": [
    "path = \"./translation2019zh_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:30:54.204599Z",
     "start_time": "2019-11-12T09:30:54.195653Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data_examples(input_file):\n",
    "    \"\"\"Read a tang poet json file into a list \"\"\"\n",
    "    data = []\n",
    "    with open(input_file, \"r\") as reader:\n",
    "        for line in reader.readlines():\n",
    "            line = line.strip()\n",
    "            line = eval(line)\n",
    "            data.append(line)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:30:55.257925Z",
     "start_time": "2019-11-12T09:30:55.243595Z"
    }
   },
   "outputs": [],
   "source": [
    "data = read_data_examples(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:30:56.172409Z",
     "start_time": "2019-11-12T09:30:56.162801Z"
    }
   },
   "outputs": [],
   "source": [
    "def padding(seq,max_len=512):\n",
    "    \"\"\"padding至batch内的最大长度\n",
    "    \"\"\"\n",
    "    ML = max_len\n",
    "    return np.array([\n",
    "        np.concatenate([x, [0] * (ML - len(x))]) if len(x) < ML else x[:max_len] for x in seq\n",
    "    ])\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    while True:\n",
    "        X, S = [], []\n",
    "        for line in data:\n",
    "            x, s = tokenizer.encode(line[\"chinese\"].lower(),line[\"english\"].lower())\n",
    "#             print(x,s)\n",
    "            X.append(x)\n",
    "            S.append(s)\n",
    "            if len(X) == batch_size:\n",
    "                X = padding(X)\n",
    "                S = padding(S)\n",
    "                yield [X, S], None\n",
    "                X, S = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:32:34.946141Z",
     "start_time": "2019-11-12T09:30:58.556218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> searching: bert/embeddings/word_embeddings, found name: bert/embeddings/word_embeddings\n",
      "==> searching: bert/embeddings/position_embeddings, found name: bert/embeddings/position_embeddings\n",
      "==> searching: bert/embeddings/token_type_embeddings, found name: bert/embeddings/token_type_embeddings\n",
      "==> searching: bert/embeddings/LayerNorm/gamma, found name: bert/embeddings/LayerNorm/gamma\n",
      "==> searching: bert/embeddings/LayerNorm/beta, found name: bert/embeddings/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_0/attention/self/query/kernel, found name: bert/encoder/layer_0/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_0/attention/self/query/bias, found name: bert/encoder/layer_0/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_0/attention/self/key/kernel, found name: bert/encoder/layer_0/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_0/attention/self/key/bias, found name: bert/encoder/layer_0/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_0/attention/self/value/kernel, found name: bert/encoder/layer_0/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_0/attention/self/value/bias, found name: bert/encoder/layer_0/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_0/attention/output/dense/kernel, found name: bert/encoder/layer_0/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_0/attention/output/dense/bias, found name: bert/encoder/layer_0/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_0/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_0/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_0/attention/output/LayerNorm/beta, found name: bert/encoder/layer_0/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_0/intermediate/dense/kernel, found name: bert/encoder/layer_0/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_0/intermediate/dense/bias, found name: bert/encoder/layer_0/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_0/output/dense/kernel, found name: bert/encoder/layer_0/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_0/output/dense/bias, found name: bert/encoder/layer_0/output/dense/bias\n",
      "==> searching: bert/encoder/layer_0/output/LayerNorm/gamma, found name: bert/encoder/layer_0/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_0/output/LayerNorm/beta, found name: bert/encoder/layer_0/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_1/attention/self/query/kernel, found name: bert/encoder/layer_1/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_1/attention/self/query/bias, found name: bert/encoder/layer_1/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_1/attention/self/key/kernel, found name: bert/encoder/layer_1/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_1/attention/self/key/bias, found name: bert/encoder/layer_1/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_1/attention/self/value/kernel, found name: bert/encoder/layer_1/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_1/attention/self/value/bias, found name: bert/encoder/layer_1/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_1/attention/output/dense/kernel, found name: bert/encoder/layer_1/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_1/attention/output/dense/bias, found name: bert/encoder/layer_1/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_1/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_1/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_1/attention/output/LayerNorm/beta, found name: bert/encoder/layer_1/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_1/intermediate/dense/kernel, found name: bert/encoder/layer_1/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_1/intermediate/dense/bias, found name: bert/encoder/layer_1/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_1/output/dense/kernel, found name: bert/encoder/layer_1/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_1/output/dense/bias, found name: bert/encoder/layer_1/output/dense/bias\n",
      "==> searching: bert/encoder/layer_1/output/LayerNorm/gamma, found name: bert/encoder/layer_1/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_1/output/LayerNorm/beta, found name: bert/encoder/layer_1/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_2/attention/self/query/kernel, found name: bert/encoder/layer_2/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_2/attention/self/query/bias, found name: bert/encoder/layer_2/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_2/attention/self/key/kernel, found name: bert/encoder/layer_2/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_2/attention/self/key/bias, found name: bert/encoder/layer_2/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_2/attention/self/value/kernel, found name: bert/encoder/layer_2/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_2/attention/self/value/bias, found name: bert/encoder/layer_2/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_2/attention/output/dense/kernel, found name: bert/encoder/layer_2/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_2/attention/output/dense/bias, found name: bert/encoder/layer_2/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_2/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_2/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_2/attention/output/LayerNorm/beta, found name: bert/encoder/layer_2/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_2/intermediate/dense/kernel, found name: bert/encoder/layer_2/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_2/intermediate/dense/bias, found name: bert/encoder/layer_2/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_2/output/dense/kernel, found name: bert/encoder/layer_2/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_2/output/dense/bias, found name: bert/encoder/layer_2/output/dense/bias\n",
      "==> searching: bert/encoder/layer_2/output/LayerNorm/gamma, found name: bert/encoder/layer_2/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_2/output/LayerNorm/beta, found name: bert/encoder/layer_2/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_3/attention/self/query/kernel, found name: bert/encoder/layer_3/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_3/attention/self/query/bias, found name: bert/encoder/layer_3/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_3/attention/self/key/kernel, found name: bert/encoder/layer_3/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_3/attention/self/key/bias, found name: bert/encoder/layer_3/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_3/attention/self/value/kernel, found name: bert/encoder/layer_3/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_3/attention/self/value/bias, found name: bert/encoder/layer_3/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_3/attention/output/dense/kernel, found name: bert/encoder/layer_3/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_3/attention/output/dense/bias, found name: bert/encoder/layer_3/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_3/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_3/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_3/attention/output/LayerNorm/beta, found name: bert/encoder/layer_3/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_3/intermediate/dense/kernel, found name: bert/encoder/layer_3/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_3/intermediate/dense/bias, found name: bert/encoder/layer_3/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_3/output/dense/kernel, found name: bert/encoder/layer_3/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_3/output/dense/bias, found name: bert/encoder/layer_3/output/dense/bias\n",
      "==> searching: bert/encoder/layer_3/output/LayerNorm/gamma, found name: bert/encoder/layer_3/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_3/output/LayerNorm/beta, found name: bert/encoder/layer_3/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_4/attention/self/query/kernel, found name: bert/encoder/layer_4/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_4/attention/self/query/bias, found name: bert/encoder/layer_4/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_4/attention/self/key/kernel, found name: bert/encoder/layer_4/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_4/attention/self/key/bias, found name: bert/encoder/layer_4/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_4/attention/self/value/kernel, found name: bert/encoder/layer_4/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_4/attention/self/value/bias, found name: bert/encoder/layer_4/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_4/attention/output/dense/kernel, found name: bert/encoder/layer_4/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_4/attention/output/dense/bias, found name: bert/encoder/layer_4/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_4/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_4/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_4/attention/output/LayerNorm/beta, found name: bert/encoder/layer_4/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_4/intermediate/dense/kernel, found name: bert/encoder/layer_4/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_4/intermediate/dense/bias, found name: bert/encoder/layer_4/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_4/output/dense/kernel, found name: bert/encoder/layer_4/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_4/output/dense/bias, found name: bert/encoder/layer_4/output/dense/bias\n",
      "==> searching: bert/encoder/layer_4/output/LayerNorm/gamma, found name: bert/encoder/layer_4/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_4/output/LayerNorm/beta, found name: bert/encoder/layer_4/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_5/attention/self/query/kernel, found name: bert/encoder/layer_5/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_5/attention/self/query/bias, found name: bert/encoder/layer_5/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_5/attention/self/key/kernel, found name: bert/encoder/layer_5/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_5/attention/self/key/bias, found name: bert/encoder/layer_5/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_5/attention/self/value/kernel, found name: bert/encoder/layer_5/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_5/attention/self/value/bias, found name: bert/encoder/layer_5/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_5/attention/output/dense/kernel, found name: bert/encoder/layer_5/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_5/attention/output/dense/bias, found name: bert/encoder/layer_5/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_5/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_5/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_5/attention/output/LayerNorm/beta, found name: bert/encoder/layer_5/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_5/intermediate/dense/kernel, found name: bert/encoder/layer_5/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_5/intermediate/dense/bias, found name: bert/encoder/layer_5/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_5/output/dense/kernel, found name: bert/encoder/layer_5/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_5/output/dense/bias, found name: bert/encoder/layer_5/output/dense/bias\n",
      "==> searching: bert/encoder/layer_5/output/LayerNorm/gamma, found name: bert/encoder/layer_5/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_5/output/LayerNorm/beta, found name: bert/encoder/layer_5/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_6/attention/self/query/kernel, found name: bert/encoder/layer_6/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_6/attention/self/query/bias, found name: bert/encoder/layer_6/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_6/attention/self/key/kernel, found name: bert/encoder/layer_6/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_6/attention/self/key/bias, found name: bert/encoder/layer_6/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_6/attention/self/value/kernel, found name: bert/encoder/layer_6/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_6/attention/self/value/bias, found name: bert/encoder/layer_6/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_6/attention/output/dense/kernel, found name: bert/encoder/layer_6/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_6/attention/output/dense/bias, found name: bert/encoder/layer_6/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_6/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_6/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_6/attention/output/LayerNorm/beta, found name: bert/encoder/layer_6/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_6/intermediate/dense/kernel, found name: bert/encoder/layer_6/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_6/intermediate/dense/bias, found name: bert/encoder/layer_6/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_6/output/dense/kernel, found name: bert/encoder/layer_6/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_6/output/dense/bias, found name: bert/encoder/layer_6/output/dense/bias\n",
      "==> searching: bert/encoder/layer_6/output/LayerNorm/gamma, found name: bert/encoder/layer_6/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_6/output/LayerNorm/beta, found name: bert/encoder/layer_6/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_7/attention/self/query/kernel, found name: bert/encoder/layer_7/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_7/attention/self/query/bias, found name: bert/encoder/layer_7/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_7/attention/self/key/kernel, found name: bert/encoder/layer_7/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_7/attention/self/key/bias, found name: bert/encoder/layer_7/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_7/attention/self/value/kernel, found name: bert/encoder/layer_7/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_7/attention/self/value/bias, found name: bert/encoder/layer_7/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_7/attention/output/dense/kernel, found name: bert/encoder/layer_7/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_7/attention/output/dense/bias, found name: bert/encoder/layer_7/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_7/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_7/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_7/attention/output/LayerNorm/beta, found name: bert/encoder/layer_7/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_7/intermediate/dense/kernel, found name: bert/encoder/layer_7/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_7/intermediate/dense/bias, found name: bert/encoder/layer_7/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_7/output/dense/kernel, found name: bert/encoder/layer_7/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_7/output/dense/bias, found name: bert/encoder/layer_7/output/dense/bias\n",
      "==> searching: bert/encoder/layer_7/output/LayerNorm/gamma, found name: bert/encoder/layer_7/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_7/output/LayerNorm/beta, found name: bert/encoder/layer_7/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_8/attention/self/query/kernel, found name: bert/encoder/layer_8/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_8/attention/self/query/bias, found name: bert/encoder/layer_8/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_8/attention/self/key/kernel, found name: bert/encoder/layer_8/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_8/attention/self/key/bias, found name: bert/encoder/layer_8/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_8/attention/self/value/kernel, found name: bert/encoder/layer_8/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_8/attention/self/value/bias, found name: bert/encoder/layer_8/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_8/attention/output/dense/kernel, found name: bert/encoder/layer_8/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_8/attention/output/dense/bias, found name: bert/encoder/layer_8/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_8/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_8/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_8/attention/output/LayerNorm/beta, found name: bert/encoder/layer_8/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_8/intermediate/dense/kernel, found name: bert/encoder/layer_8/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_8/intermediate/dense/bias, found name: bert/encoder/layer_8/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_8/output/dense/kernel, found name: bert/encoder/layer_8/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_8/output/dense/bias, found name: bert/encoder/layer_8/output/dense/bias\n",
      "==> searching: bert/encoder/layer_8/output/LayerNorm/gamma, found name: bert/encoder/layer_8/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_8/output/LayerNorm/beta, found name: bert/encoder/layer_8/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_9/attention/self/query/kernel, found name: bert/encoder/layer_9/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_9/attention/self/query/bias, found name: bert/encoder/layer_9/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_9/attention/self/key/kernel, found name: bert/encoder/layer_9/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_9/attention/self/key/bias, found name: bert/encoder/layer_9/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_9/attention/self/value/kernel, found name: bert/encoder/layer_9/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_9/attention/self/value/bias, found name: bert/encoder/layer_9/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_9/attention/output/dense/kernel, found name: bert/encoder/layer_9/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_9/attention/output/dense/bias, found name: bert/encoder/layer_9/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_9/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_9/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_9/attention/output/LayerNorm/beta, found name: bert/encoder/layer_9/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_9/intermediate/dense/kernel, found name: bert/encoder/layer_9/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_9/intermediate/dense/bias, found name: bert/encoder/layer_9/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_9/output/dense/kernel, found name: bert/encoder/layer_9/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_9/output/dense/bias, found name: bert/encoder/layer_9/output/dense/bias\n",
      "==> searching: bert/encoder/layer_9/output/LayerNorm/gamma, found name: bert/encoder/layer_9/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_9/output/LayerNorm/beta, found name: bert/encoder/layer_9/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_10/attention/self/query/kernel, found name: bert/encoder/layer_10/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_10/attention/self/query/bias, found name: bert/encoder/layer_10/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_10/attention/self/key/kernel, found name: bert/encoder/layer_10/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_10/attention/self/key/bias, found name: bert/encoder/layer_10/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_10/attention/self/value/kernel, found name: bert/encoder/layer_10/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_10/attention/self/value/bias, found name: bert/encoder/layer_10/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_10/attention/output/dense/kernel, found name: bert/encoder/layer_10/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_10/attention/output/dense/bias, found name: bert/encoder/layer_10/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_10/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_10/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_10/attention/output/LayerNorm/beta, found name: bert/encoder/layer_10/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_10/intermediate/dense/kernel, found name: bert/encoder/layer_10/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_10/intermediate/dense/bias, found name: bert/encoder/layer_10/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_10/output/dense/kernel, found name: bert/encoder/layer_10/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_10/output/dense/bias, found name: bert/encoder/layer_10/output/dense/bias\n",
      "==> searching: bert/encoder/layer_10/output/LayerNorm/gamma, found name: bert/encoder/layer_10/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_10/output/LayerNorm/beta, found name: bert/encoder/layer_10/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_11/attention/self/query/kernel, found name: bert/encoder/layer_11/attention/self/query/kernel\n",
      "==> searching: bert/encoder/layer_11/attention/self/query/bias, found name: bert/encoder/layer_11/attention/self/query/bias\n",
      "==> searching: bert/encoder/layer_11/attention/self/key/kernel, found name: bert/encoder/layer_11/attention/self/key/kernel\n",
      "==> searching: bert/encoder/layer_11/attention/self/key/bias, found name: bert/encoder/layer_11/attention/self/key/bias\n",
      "==> searching: bert/encoder/layer_11/attention/self/value/kernel, found name: bert/encoder/layer_11/attention/self/value/kernel\n",
      "==> searching: bert/encoder/layer_11/attention/self/value/bias, found name: bert/encoder/layer_11/attention/self/value/bias\n",
      "==> searching: bert/encoder/layer_11/attention/output/dense/kernel, found name: bert/encoder/layer_11/attention/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_11/attention/output/dense/bias, found name: bert/encoder/layer_11/attention/output/dense/bias\n",
      "==> searching: bert/encoder/layer_11/attention/output/LayerNorm/gamma, found name: bert/encoder/layer_11/attention/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_11/attention/output/LayerNorm/beta, found name: bert/encoder/layer_11/attention/output/LayerNorm/beta\n",
      "==> searching: bert/encoder/layer_11/intermediate/dense/kernel, found name: bert/encoder/layer_11/intermediate/dense/kernel\n",
      "==> searching: bert/encoder/layer_11/intermediate/dense/bias, found name: bert/encoder/layer_11/intermediate/dense/bias\n",
      "==> searching: bert/encoder/layer_11/output/dense/kernel, found name: bert/encoder/layer_11/output/dense/kernel\n",
      "==> searching: bert/encoder/layer_11/output/dense/bias, found name: bert/encoder/layer_11/output/dense/bias\n",
      "==> searching: bert/encoder/layer_11/output/LayerNorm/gamma, found name: bert/encoder/layer_11/output/LayerNorm/gamma\n",
      "==> searching: bert/encoder/layer_11/output/LayerNorm/beta, found name: bert/encoder/layer_11/output/LayerNorm/beta\n",
      "==> searching: cls/predictions/transform/dense/kernel, found name: cls/predictions/transform/dense/kernel\n",
      "==> searching: cls/predictions/transform/dense/bias, found name: cls/predictions/transform/dense/bias\n",
      "==> searching: cls/predictions/transform/LayerNorm/gamma, found name: cls/predictions/transform/LayerNorm/gamma\n",
      "==> searching: cls/predictions/transform/LayerNorm/beta, found name: cls/predictions/transform/LayerNorm/beta\n",
      "==> searching: cls/predictions/output_bias, found name: cls/predictions/output_bias\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (Embedding)     (None, None, 768)    81315072    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Sequence-Mask (Lambda)          (None, None)         0           Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Attention-Mask (Lambda)         (None, None, None)   0           Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Embedding-Dropout[0][0]          \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 768)    0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 768)    0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, None, 768)    0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, None, 768)    0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, None, 768)    0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, None, 768)    0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, None, 768)    0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, None, 768)    0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, None, 768)    4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, None, 768)    0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, None, 768)    1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, None, 768)    0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, None, 768)    1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, None, 768)    0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, None, 768)    1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Sequence-Mask[0][0]              \n",
      "                                                                 Attention-Mask[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, None, 768)    4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, None, 768)    0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, None, 768)    1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, None, 768)    590592      Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, None, 768)    1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Proba (EmbeddingDense)      (None, None, 105879) 105879      MLM-Norm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 167,463,831\n",
      "Trainable params: 167,463,831\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/developer/wp/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training_utils.py:819: UserWarning: Output MLM-Proba missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to MLM-Proba.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "model = build_bert_model(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    application='seq2seq',\n",
    "    # 只保留keep_words中的字，精简原字表\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 交叉熵作为loss，并mask掉输入部分的预测\n",
    "y_in = model.input[0][:, 1:]  # 目标tokens\n",
    "y_mask = model.input[1][:, 1:]\n",
    "y = model.output[:, :-1]  # 预测tokens，预测与目标错开一位\n",
    "cross_entropy = K.sparse_categorical_crossentropy(y_in, y)\n",
    "cross_entropy = K.sum(cross_entropy * y_mask) / K.sum(y_mask)\n",
    "model.add_loss(cross_entropy)\n",
    "model.compile(optimizer=Adam(1e-5))\n",
    "# from keras.utils.training_utils import multi_gpu_model   #导入keras多GPU函数\n",
    "# parallel_model = multi_gpu_model(model, gpus=3)#设置使用2个gpu，该句放在模型compile之前\n",
    "# parallel_model.compile(optimizer=Adam(1e-5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:43:34.259070Z",
     "start_time": "2019-11-12T09:43:34.243991Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_sent(s, topk=3):\n",
    "    \"\"\"beam search解码\n",
    "    每次只保留topk个最优候选结果；如果topk=1，那么就是贪心搜索\n",
    "    \"\"\"\n",
    "    token_ids, segment_ids = tokenizer.encode(s[:max_input_len])\n",
    "    target_ids = [[] for _ in range(topk)]  # 候选答案id\n",
    "    target_scores = [0] * topk  # 候选答案分数\n",
    "    for i in range(max_output_len):  # 强制要求输出不超过max_output_len字\n",
    "        _target_ids = [token_ids + t for t in target_ids]\n",
    "#         print(\"_target_ids\",_target_ids)\n",
    "        _segment_ids = [segment_ids + [1] * len(t) for t in target_ids]\n",
    "#         print(\"_segment_ids\",_segment_ids)\n",
    "        _probas = model.predict([_target_ids, _segment_ids\n",
    "                                 ])[: ,-1,3:]  # 直接忽略[PAD], [UNK], [CLS]\n",
    "#         print(\"_probas\",_probas)\n",
    "        _log_probas = np.log(_probas + 1e-6)  # 取对数，方便计算\n",
    "        _topk_arg = _log_probas.argsort(axis=1)[:, -topk:]  # 每一项选出topk\n",
    "        _candidate_ids, _candidate_scores = [], []\n",
    "        for j, (ids, sco) in enumerate(zip(target_ids, target_scores)):\n",
    "            # 预测第一个字的时候，输入的topk事实上都是同一个，\n",
    "            # 所以只需要看第一个，不需要遍历后面的。\n",
    "            if i == 0 and j > 0:\n",
    "                continue\n",
    "            for k in _topk_arg[j]:\n",
    "                _candidate_ids.append(ids + [k + 3])\n",
    "                _candidate_scores.append(sco + _log_probas[j][k])\n",
    "        _topk_arg = np.argsort(_candidate_scores)[-topk:]  # 从中选出新的topk\n",
    "        target_ids = [_candidate_ids[k] for k in _topk_arg]\n",
    "        target_scores = [_candidate_scores[k] for k in _topk_arg]\n",
    "#         print(\"target_scores\", target_scores)\n",
    "        best_one = np.argmax(target_scores)\n",
    "        if target_ids[best_one][-1] == token_dict.get(\"[SEP]\"):\n",
    "            return tokenizer.decode(target_ids[best_one])\n",
    "        \n",
    "    # 如果max_output_len字都找不到结束符，直接返回\n",
    "    return tokenizer.decode(target_ids[np.argmax(target_scores)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105879"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T09:43:58.500008Z",
     "start_time": "2019-11-12T09:43:39.975581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译结果: when we learn about python web development, we choose to use django and flask frameworks.\n"
     ]
    }
   ],
   "source": [
    "def just_show():\n",
    "    s1 = \"我们学习python Web开发时，会选择使用Django、flask等框架。\"\n",
    "    print('翻译结果:', gen_sent(s1.lower()))\n",
    "    \n",
    "    \n",
    "    \n",
    "class Evaluate(Callback):\n",
    "    def __init__(self):\n",
    "        self.lowest = 1e10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 保存最优\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save_weights('./best_trans.weights')\n",
    "        # 演示效果\n",
    "        just_show()\n",
    "\n",
    "\n",
    "batch_size = 2\n",
    "steps_per_epoch = 2000\n",
    "epochs = 100\n",
    "max_input_len = 256\n",
    "max_output_len = 256\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    evaluator = Evaluate()\n",
    "\n",
    "#     model.fit_generator(data_generator(),\n",
    "#                         steps_per_epoch=steps_per_epoch,\n",
    "#                         epochs=epochs,\n",
    "#                         callbacks=[evaluator])\n",
    "    model.load_weights(\"best_trans.weights\")\n",
    "    just_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
